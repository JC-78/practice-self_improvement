{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af83019c-ada5-4509-95e2-730153e40dcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# CMU 10405/10605 Machine Learning with Large Datasets\n",
    "## **Running Your First Notebook**\n",
    "This notebook will show you how to install the course libraries, create your first Spark cluster, and test basic notebook functionality.  To move through the notebook just run each of the cells.  You will not need to solve any problems to complete this lab.  You can run a cell by pressing \"shift-enter\", which will compute the current cell and advance to the next cell, or by clicking in a cell and pressing \"control-enter\", which will compute the current cell and remain in that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c13f214-e51a-4741-a85c-7122d0a37f68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### ** Part 1: Attach and Test Class helper Library **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a6f60283-dc4a-4fc6-a263-6163958c1cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### (1a) Install class helper library into your Databricks CE workspace\n",
    "- The class helper library is called \"nose\"\n",
    "- You can install the library into your workspace following the following instructions:\n",
    " - Step 1: Click on \"Workspace\", then on the dropdown and select \"Create\" and \"Library\"\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/Lab0_Library1.png\" alt=\"Drawing\" />\n",
    " - Step 2 Enter the name of the library by selecting \"Upload Python Egg or PyPI\" and entering \"nose\" in the \"PyPI Name\" field\n",
    "\n",
    "\n",
    " - Step 3 Make sure the checkbox for auto-attaching the library to your cluster is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca8a25da-7772-4e4f-9f27-7d07425f7948",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### ** Part 2: Test Spark functionality **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c561e79d-c8ec-41c7-9266-14412a268eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** (2a) Create a DataFrame and filter it **\n",
    "\n",
    "When you run the next cell (with control-enter or shift-enter), you will see the following popup.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/Lab0_Cluster.png\" alt=\"Drawing\" />\n",
    "\n",
    "Select the click box and then \"Launch and Run\". The display at the top of your notebook will change to \"Pending\"\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/Lab0_Cluster_Pending.png\" alt=\"Drawing\" />\n",
    "\n",
    "Note that it may take a few seconds to a few minutes to start your cluster. Once your cluster is running the display will changed to \"Attached\"\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/Lab0_Cluster_Attached.png\" alt=\"Drawing\" />\n",
    "\n",
    "Congratulations! You just launched your Spark cluster in the cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a2646ad1-4019-47e8-8c0a-500836197d12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='Bill', age=4)]\n"
     ]
    }
   ],
   "source": [
    "# Check that Spark is working\n",
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#When using pyspark as a library, the sqlContext is not created automatically, \n",
    "#you have to create it yourself:\n",
    "sqlContext = pyspark.SQLContext(pyspark.SparkContext())\n",
    "\n",
    "data = [('Alice', 1), ('Bob', 2), ('Bill', 4)]\n",
    "df = sqlContext.createDataFrame(data, ['name', 'age'])\n",
    "fil = df.filter(df.age > 3).collect()\n",
    "print(fil)\n",
    "\n",
    "# If the Spark job doesn't work properly this will raise an AssertionError\n",
    "assert(fil == [Row(u'Bill', 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f92df68-c8ed-4376-add4-b08883ffa877",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate(); #this helps avoid issue of multiple sparkcontext\n",
    "\n",
    "peopleRDD = sc.parallelize([\"bob\",\"alice\",\"bill\"])\n",
    "words_filter = peopleRDD.filter(lambda x: \"i\" in x)\n",
    "words_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "292a7325-48eb-42f1-a520-2598e8f340ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alice', 'bill']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = words_filter.collect()\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f1df639-a9e9-4b8d-bcbf-75fd800f17fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "peopleRDD.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5533661f-7b59-45d1-b158-2a92036fee54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bob', 'alice']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c411c6d-6977-41f6-bc0d-dc326e763e0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bob', 'alice', 'bill']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00992482-ec99-4191-b540-4291bee611b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** (2b) Loading a text file **\n",
    "\n",
    "Let's load a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e4e38f9-b4dd-4c05-94a1-74c00e54b57f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/var/folders/22/2sr4vq913y9_4v5xpn7h9fqm0000gn/T/spark-2c01587e-64ef-495c-a5d1-ef5f1290eba9/userFiles-1946d2a4-8238-47bf-8086-a7f671c7c24d/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/10605/data/master/hw1/shakespeare.txt\"\n",
    "sc.addFile(url)\n",
    "\n",
    "# verifying that the file got downloaded\n",
    "print(SparkFiles.get(\"shakespeare.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c762b94a-9af1-41d3-9d86-0ae61aa690a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the file are: 122395\n"
     ]
    }
   ],
   "source": [
    "file_lines_count = len(open(SparkFiles.get(\"shakespeare.txt\")).readlines())\n",
    "print(f'Number of lines in the file are: {file_lines_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3b7a479-a43e-47e8-8b50-c6aeafab55cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122395\n"
     ]
    }
   ],
   "source": [
    "shakespeare_rdd = sc.textFile(\"file://\" + SparkFiles.get(\"shakespeare.txt\"))\n",
    "shakespeare_rdd_lines = shakespeare_rdd.count()\n",
    "\n",
    "print(shakespeare_rdd_lines)\n",
    "\n",
    "assert(shakespeare_rdd_lines == file_lines_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29a73ee7-b880-4b84-8f2f-6ce3577b48b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122395\n"
     ]
    }
   ],
   "source": [
    "# Check loading data with sqlContext.read.text\n",
    "dataDF = sqlContext.read.text(\"file://\" + SparkFiles.get(\"shakespeare.txt\"))\n",
    "shakespeareCount = dataDF.count()  # number of rows in dataDF\n",
    "\n",
    "print(shakespeareCount)\n",
    "\n",
    "# If the text file didn't load properly an AssertionError will be raised\n",
    "assert(shakespeareCount == file_lines_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d72a025e-b6f8-47a9-bb1b-fa9f735b6bb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### ** Part 3: Test class testing library **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a72a9a48-c824-4462-8504-e4eed24dc937",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** (3a) Compare with numbers **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29997882-2640-49fe-9e23-26bdc009b8dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-73067007bd45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Check the nose library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This should not print out anything if the test is passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massert_true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtwelve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nose'"
     ]
    }
   ],
   "source": [
    "# TEST Compare with numbers\n",
    "# Check the nose library\n",
    "# This should not print out anything if the test is passed\n",
    "from nose.tools import assert_equal, assert_true\n",
    "\n",
    "twelve = 12\n",
    "assert_equal(twelve, 12, 'twelve should equal 12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da402c11-52db-4d92-8e22-f9088a3ab494",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** (3b) Compare lists **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0275406d-5469-40dd-ad49-28082f8bcf39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST Compare lists (2b)\n",
    "# This should not print out anything if the test is passed\n",
    "unsortedList = [(5, 'b'), (5, 'a'), (4, 'c'), (3, 'a')]\n",
    "assert_equal(sorted(unsortedList), [(3, 'a'), (4, 'c'), (5, 'a'), (5, 'b')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a1e385bd-888a-4bdf-a5c7-e023757c801f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### ** Part 4: Check MathJax formulas **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e987392-78a8-4572-9406-14fbd360d2de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** (4a) Gradient descent formula **\n",
    "\n",
    "You should see a formula on the line below this one: \\\\[ \\scriptsize \\mathbf{w}_{i+1} = \\mathbf{w}_i - \\alpha_i \\sum_j (\\mathbf{w}_i^\\top\\mathbf{x}_j  - y_j) \\mathbf{x}_j \\,.\\\\]\n",
    "\n",
    "This formula is included inline with the text and is \\\\( \\scriptsize (\\mathbf{w}^\\top \\mathbf{x} - y) \\mathbf{x} \\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57b665d2-9b6f-4375-9b0b-65c0967e4381",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** (4b) Log loss formula **\n",
    "\n",
    "This formula shows log loss for single point. Log loss is defined as: \\\\[  \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "beea39ef-1fab-4d42-9941-0c6a4254f9de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Spark Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca5deafe-c6ac-450c-9ef2-82b38787541f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### ** Part 5: Basic Spark Transformations and Actions **\n",
    "\n",
    "The remaining notebook will walk you through a basic Spark transformations and actions.\n",
    "\n",
    "Refer to the Spark guide: https://spark.apache.org/docs/2.4.5/rdd-programming-guide.html\n",
    "\n",
    "Refer to last week's recitation slides and video: https://10605.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d875c6d6-a69f-4621-bde2-4947fc0f5387",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a sample list\n",
    "my_list = [i for i in range(0, 10000000)]\n",
    "\n",
    "# Parallelize the data\n",
    "rdd_0 = sc.parallelize(my_list, 4)\n",
    "\"\"\"\n",
    "One important parameter for parallel collections is the number of partitions to cut \n",
    "the dataset into. Spark will run one task for each partition of the cluster. \n",
    "Typically you want 2-4 partitions for each CPU in your cluster. \n",
    "Normally, Spark tries to set the number of partitions automatically based on your \n",
    "cluster. However, you can also set it manually by passing it as a second parameter\n",
    "to parallelize (e.g. sc.parallelize(data, 10)).\n",
    "\"\"\"\n",
    "\n",
    "# Add 4 to each element\n",
    "rdd_1 = rdd_0.map(lambda x: x + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81f14835-d4f5-4a24-9140-1b18cf72a960",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1.1. What will the following line print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba45317c-5303-45de-b7d2-b1b9d6b119d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[23] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b946a9c3-b7f4-4642-9df6-8f351e0b0f34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[24] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "582b6242-cbb3-451b-b07f-ace9a32d3e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#rdd_1.collect()\n",
    "#rdd_1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4907c767-9261-4e3e-bb0e-92e6a54098fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Remember that Spark does lazy evaluation, and only exectues things once actions are triggered.\n",
    "Execute an *action* and actually print values on the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff8b73e3-96a7-4803-888e-08c4b1a69539",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1.2. Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b35f1e2-6b93-46dc-933f-bfa5fc21847d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', [10405, 15213, 10301]),\n",
       " ('Bob', [10405, 10701]),\n",
       " ('Chad', [15513, 15445, 10405, 15213])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students = [('Alice', [10405, 15213, 10301]), ('Bob', [10405, 10701]), ('Chad', [15513, 15445, 10405, 15213])]\n",
    "\n",
    "rdd_students = sc.parallelize(students)\n",
    "\n",
    "rdd_students.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3eacfc96-e52d-44fa-8343-5984a50979a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How do we find the distinct courses students are taking?\n",
    "# The equivalent of doing: set([course for student in students for course in student[1]]) in Python\n",
    "# It should be: {10301, 10405, 10701, 15213, 15445, 15513}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f7dd0a79-fd75-4229-a701-e1e510fc85ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_students.flatMap(lambda x:x[1])\n",
    "#rdd_students.flatMap(lambda x:x[1]).distinct().collect()\n",
    "\n",
    "#PySpark flatMap() is a transformation operation that flattens the RDD/DataFrame \n",
    "#(array/map DataFrame columns) after applying the function on every element and \n",
    "#returns a new PySpark RDD/DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7fcb1cbd-339f-4cf6-bc19-39382f144d95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1.3. Lets now work with a PairRDD, that is, an RDD of (key, value)\n",
    "\n",
    "+--+------+-----------+  \n",
    "|id|league|height (cm)|  \n",
    "+--+------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4664136a-1c69-4392-9ffd-f3078fbc1a4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'CMU', 192),\n",
       " (2, 'CMU', 218),\n",
       " (3, 'CMU', 195),\n",
       " (4, 'NBA', 192),\n",
       " (5, 'NBA', 198),\n",
       " (6, 'CMU', 166),\n",
       " (7, 'NBA', 195),\n",
       " (8, 'NBA', 182),\n",
       " (9, 'CMU', 189),\n",
       " (10, 'NBA', 180)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = [(1, 'CMU', 192), (2, 'CMU', 218), (3, 'CMU', 195), (4, 'NBA', 192), (5,'NBA', 198)]\n",
    "players += [(6, 'CMU', 166), (7, 'NBA', 195), (8, 'NBA', 182), (9, 'CMU', 189), (10,'NBA', 180)]\n",
    "players += [(11, 'NBA', 190), (12, 'CMU', 195), (13, 'CMU', 195), (14, 'NBA', 197), (15,'NBA', 195)]\n",
    "rdd_players = sc.parallelize(players)\n",
    "\n",
    "rdd_players.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fe3a5ae-4628-492e-8f99-4d46b01a5266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2879"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of all heights\n",
    "\n",
    "def sum_fun(x,y):\n",
    "  return x + y\n",
    "\n",
    "#example: sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
    "#15 \n",
    "\n",
    "rdd_players.map(lambda x:x[2]).reduce(lambda x,y: x+y)\n",
    "#OR\n",
    "#rdd_players.map(lambda x:x[2]).reduce(sum_fun)\n",
    "#OR\n",
    "#rdd_players.map(lambda x:x[2]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7faa2e61-8f70-4bd0-8361-a4425096c512",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CMU', 1350), ('NBA', 1529)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of heights per league\n",
    "\n",
    "sum_heights = rdd_players.map(lambda x:(x[1],x[2])).reduceByKey(lambda x,y: x+y)\n",
    "sum_heights.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "108b3912-4239-4a1d-8714-d39415f8bc19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CMU', 7), ('NBA', 8)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of player per league\n",
    "\n",
    "count_players_rdd = rdd_players.map(lambda x:(x[1],1)).reduceByKey(lambda x,y : x+y)\n",
    "#count_players_rdd = rdd_players.map(lambda x:(x[1],x[2])).countByKey()\n",
    "count_players_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cbbd128-8105-468f-ab01-ad54d8320ffd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CMU', 192.85714285714286), ('NBA', 191.125)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average height per league\n",
    "\n",
    "#sum_heights.join(count_players_rdd).collect()\n",
    "sum_heights.join(count_players_rdd).mapValues(lambda x : x[0]/x[1]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4244e509-be59-4e2a-b85f-9fd41f61d5c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "recitation_1_fall21",
   "notebookOrigID": 378849363066898,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
